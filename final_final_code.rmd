---
title: "Final Team 3"
author: "LD"
date: "2024-06-20"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r, warning=FALSE, message=FALSE}
library(AppliedPredictiveModeling)
library(e1071)
library(caret)
library(rpart)
library(rpart.plot)
library(partykit)
library(earth)
library(kernlab)
library(mlbench)
library(randomForest)
library(dplyr)
library(corrplot)
library(pROC)
library(RANN)
library(glmnet)
library(ggplot2)
library(igraph)
library(tidyverse)
library(gt)
library(RCurl)
library(reshape2)
```

```{r}
# Read data
url <- "https://github.com/renaqd/team3-ADS503/raw/main/data.csv"
changer_data<- read.csv(url, header = TRUE) %>%
  mutate(Class = as.factor(Class))

```

```{r}

# identify NA columns
colSums(is.na(changer_data))

```

```{r}
# Skewedness
skewed<- length(1177)
for(i in 1:1177){
  skewed[i]<- skewness(changer_data[,i], na.rm = TRUE, type = 1)
}

# See where most of the data falls in range
boxplot(skewed)
```


```{r}

# subset predictors with high variability
selected_vars <- c("VR2_Dt", "GATS1c", "BCUTp.1l", "BCUTp.1h", "topoDiameter", 
                   "AATS8p", "MDEC.22", "MDEC.23")
subset_data <- changer_data %>% select(all_of(selected_vars))

```

```{r}

# data summary for variables with high variability
print(summary(subset_data))
   
```

Variables with high variability can be useful in providing insights and implications for the dataset.
The following are the reasons why the below variables were chosen:

Wide Range of Values:
Variables with high variability exhibit a wide range of values. This indicates that the data points are spread out over a larger range, which can be useful for capturing diverse patterns and relationships within the data.

Potential for Differentiation:
High variability suggests that these variables might be good candidates for differentiating between classes or groups. If the variability is related to the target variable (e.g., class labels), these variables might have high discriminative power.

Presence of Outliers:
High variability often implies the presence of outliers or extreme values. Outliers can provide important information about anomalies or rare events but might also skew the results if not handled properly.

Impact on Model Training:
Variables with high variability can impact model training in various ways. Models might need to account for this variability to make accurate predictions. Regularization techniques might be necessary to prevent overfitting.

High Variability Variables:

VR2_Dt:

Mean: 10392.88
Standard Deviation: This is likely very high given the mean and the spread of the values.
Range: The minimum value is 6.77, and the maximum value is 38724.00, indicating a very high variability.
Notable Characteristics: Very high mean and a wide range suggest significant variability.

GATS1c:

Mean: 1.394
Standard Deviation: The values are somewhat close to each other but still show some spread.
Range: The minimum value is 1.039, and the maximum value is 1.698, indicating a moderate range.
Notable Characteristics: Large range from minimum to maximum.

BCUTp.1l:

Mean: -0.3098
Standard Deviation: There is variability around the mean.
Range: The minimum value is -0.4220, and the maximum value is 0.0560, indicating a significant range.
Notable Characteristics: Significant range indicating variability around the mean.

BCUTp.1h:

Mean: 0.2491
Standard Deviation: There is variability around the mean.
Range: The minimum value is 0.0560, and the maximum value is 0.7333, indicating a significant range.
Notable Characteristics: Significant range indicating variability around the mean.

topoDiameter:

Mean: 17.14
Standard Deviation: This shows variability given the spread of the values.
Range: The minimum value is 9.00, and the maximum value is 44.00, indicating a significant range.
Notable Characteristics: Significant range from minimum to maximum.

AATS8p:

Mean: 1.439
Standard Deviation: Shows some spread in the values.
Range: The minimum value is 1.088, and the maximum value is 2.589, indicating a considerable range.
Notable Characteristics: Values spread over a considerable range.

MDEC.22:

Mean: 16.55
Standard Deviation: This value indicates variability around the mean.
Range: The minimum value is 0.00, and the maximum value is 76.00, showing a high range.
Notable Characteristics: High variability with a significant range.

MDEC.23:

Mean: 40.87
Standard Deviation: This value indicates variability around the mean.
Range: The minimum value is 16.57, and the maximum value is 80.00, indicating a high range.
Notable Characteristics: High variability with a significant range.

```{r}

# calculate the correlation matrix for the subset
subset_cor_matrix <- cor(subset_data, use = "complete.obs")

# visualize using ggplot2
melted_subset_cor_matrix <- melt(subset_cor_matrix)
ggplot(data = melted_subset_cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Correlation") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 7, hjust = 1),
        axis.title.x = element_blank(), 
        axis.title.y = element_blank()) + 
  coord_fixed()

```

```{r}

# histograms
subset_data %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal()

# density plots
subset_data %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value, fill = variable)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ variable, scales = "free") +
  theme_minimal()

# box Plots
subset_data %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = variable, y = value)) +
  geom_boxplot(fill = "blue", color = "black", alpha = 0.7) +
  theme_minimal() +
  facet_wrap(~ variable, scales = "free")

# scatter Plot Matrix
pairs(subset_data, pch = 21, bg = c("red", "green3", "blue")[changer_data$Class])

```

```{r}

# outliers
outliers <- apply(subset_data, 2, function(x) {
  Q1 <- quantile(x, 0.25)
  Q3 <- quantile(x, 0.75)
  IQR <- Q3 - Q1
  x < (Q1 - 1.5 * IQR) | x > (Q3 + 1.5 * IQR)
})

```

```{r warning=FALSE, message=FALSE, results="hide"}
#| echo: false

# identify numerical features
numerical_features <- changer_data[, sapply(changer_data, is.numeric)]

# print distribution for numerical features to PDF <<-- already done and found in GitHub
pdf("numerical_distributions.pdf")
lapply(names(numerical_features), function(feature) {
    ggplot(changer_new, aes_string(x = feature)) + 
        geom_histogram(bins = 30, fill = "blue", color = "black") + 
        ggtitle(paste("Distribution of", feature))
})
dev.off()

```

```{r}

# class distribution <<- code above shows numeric values only, this is a visual
class_distribution <- table(changer_new$Class)
barplot(class_distribution, main = "Class Distribution", col = "cyan",
        xlab = "Class", ylab = "Frequency")

```


## Try Lasso
```{r}
#| cache: true

# Define pre-processing steps
prep_steps <- c("nzv", "center", "scale")

# Set up training control with repeated k-fold cross-validation
ctrl <- trainControl(method = "repeatedcv",    # Use repeated k-fold cross-validation
                     number = 10,              # Number of folds
                     repeats = 5,              # Number of repeats
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

# Create the lambda grid for lasso
lambda_grid <- expand.grid(alpha = 1, lambda = seq(0.005, 0.05, length = 20))

# Set a random seed for reproducibility
rseed <- 503
set.seed(rseed)

# Train the Lasso model
lasso_model <- train(x = changer_data[,1:1177],
                     y = changer_data$Class,
                     method = "glmnet",
                     tuneGrid = lambda_grid,
                     preProcess = prep_steps,
                     trControl = ctrl,
                     metric = "ROC")

# Output the best tuning parameters
print(lasso_model$bestTune)

# Extract the best model's lambda value
best_lambda <- lasso_model$bestTune$lambda

# Extract the coefficients for the best lambda value
lasso_coef <- coef(lasso_model$finalModel, s = best_lambda)

# Convert coefficients to a data frame
lasso_coef_df <- as.data.frame(as.matrix(lasso_coef))

# Identify the features with non-zero coefficients
selected_features <- rownames(lasso_coef_df)[lasso_coef_df$s1 != 0]
selected_features <- selected_features[selected_features != "(Intercept)"] # Remove the intercept if present

# Print the selected features
print(selected_features)
```

```{r}
subset<- changer_data %>% 
  select(all_of(selected_features))

corr<- cor(subset)
high_corr <- findCorrelation(corr, cutoff = 0.75)
subset_new<- subset[,-high_corr]

col_nam<- (colnames(subset_new))
```

```{r}
lasso_imp<-varImp(lasso_model)
plot(lasso_imp, top = 10)


short<- c("GATS4c","SpMin3_Bhp", "MATS4e", "maxHBint10", "SpMax2_Bhp", "maxaasN", "ETA_BetaP_s", "maxHBint6")

data<- subset %>% 
  select(all_of(short)) %>%
  cbind(changer_data$Class) %>%
  rename("Class" = "changer_data$Class")
```



**Model Building Strategies**
```{r warning=FALSE, message=FALSE}
# Function for Training Models
class_function<- function(method){
  model<- train(Class ~.,
               data = data,
               method = method,
               preProc = c("center", "scale"),
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with TuneGrids
class_grid<- function(method, grid){
  model<- train(Class ~.,
                data = data,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models for Neural Networks
class_nnet<- function(method, grid, maxit){
  model<- train(Class ~.,
                data = data,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               maxit = maxit,
               trace = FALSE, 
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with KNN
class_length<- function(method, length){
  model<- train(Class ~.,
                data = data,
               method = method,
               preProc = c("center", "scale"),
               tuneLength = length,
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with Bagging
class_bag<- function(method, nbagg){
  model<- train(Class ~.,
                data = data,
               method = method,
               preProc = c("center", "scale"),
               nbagg = nbagg,
               metric = "ROC",
               trControl = ctrl)
}

# Function for Training Models with Random Forest
class_rf<- function(method, grid, ntree){
  model<- train(Class ~.,
                data = data,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               ntree = ntree,
               metric = "ROC",
               trControl = ctrl)
}

```

**Hyper Parameter Tuning Defined in Grids**
```{r warning=FALSE, message=FALSE}
set.seed(720)

# TuneGrid for Penalized Logistic Regrssion
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                        lambda = seq(.1, .4, length = 10))

# NSC TuneGrid
nsc_grid<- data.frame(threshold = seq(0, 25, length = 30))


# NNET TuneGrid #########################################################
nnet_grid <- expand.grid(size=1:4, decay=c(0, 0.1, 0.3, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4))

# SVM TuneGrid
sigmaEst <- kernlab::sigest(as.matrix(data[,1:8]))
svmgrid <- expand.grid(sigma = sigmaEst, C = 2^seq(-4,+4))

# RF TuneGrid
mtryValues <- data.frame(mtry = seq(0,8,1))

```

**Train Models**
```{r warning=FALSE, message=FALSE}
#| cache: true
rseed <- 503
set.seed(rseed)

# Generalized Linear Model: Logistic Regression
set.seed(rseed)
lr<- class_function("glm")

# Linear Discriminant Analysis
set.seed(rseed)
lda<- class_function("lda")

# Penalized LR
set.seed(rseed)
glmn<- class_grid("glmnet", glmnGrid)
plot(glmn, main = "Hyperparameter Tuning: Penalized Logistic")

# Nearest Shrunken Centroids
set.seed(rseed)
nsc<- class_grid("pam", nsc_grid)
plot(nsc, main = "Hyperparameter Tuning: Nearest Shrunken Centroid")

# svmR
set.seed(rseed)
svmR<- class_grid("svmRadial", svmgrid)
plot(svmR, main = "Hyperparameter Tuning: Support Vector Machine")

# NNET
set.seed(rseed)
nnet<- class_nnet("nnet", nnet_grid, 100)
plot(nnet, main = "Hyperparameter Tuning: Neural Network")

# K Nearest Neighbors
set.seed(rseed)
knn<- class_length("knn", 20)
plot(knn, main = "Hyperparameter Tuning: K-Nearest Neighbors")

# Bagging
set.seed(rseed)
bag<- class_bag("treebag", 50)

# Random Forest
set.seed(rseed)
rf<- class_rf("rf", mtryValues, 100)
plot(rf, main = "Hyperparameter Tuning: Random Forest")
```

```{r, warning=FALSE, message=FALSE}
model_roc <- function(model){
  roc(response = model$pred$obs,
             predictor = model$pred$Changer,
             levels = rev(levels(model$pred$obs)))
}

lrROC <- model_roc(lr)
ldaROC <- model_roc(lda)
glmnROC <- model_roc(glmn)
nscROC <- model_roc(nsc)
svmRROC <- model_roc(svmR)
knnROC <- model_roc(knn)
bagROC <- model_roc(bag)
rfROC <- model_roc(rf)
nnetROC <- model_roc(nnet)


train_re<- resamples(list(
  lr = lr,
  lda = lda,
  glmn = glmn,
  nsc = nsc,
  svmR = svmR,
  knn = knn,
  bag = bag,
  rf = rf,
  nnet = nnet
))

dotplot(train_re, metric = "ROC", main = "Confidence Intervals for Repeated CV")

```



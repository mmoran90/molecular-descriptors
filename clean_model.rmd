---
title: "Cleaning and Classifications"
author: "Madeline Chang"
date: "5/29/2024"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r, message = FALSE}
library(AppliedPredictiveModeling)
library(e1071)
library(caret)
library(rpart)
library(rpart.plot)
library(partykit)
library(earth)
library(kernlab)
library(mlbench)
library(randomForest)
library(dplyr)
library(corrplot)
library(pROC)
library(RANN)
library(glmnet)
library(gt)
library(RCurl)
```

```{r}
url <- "https://github.com/renaqd/team3-ADS503/raw/main/data.csv"
changer_data<- read.csv(url)
```

```{r}
# Check for and remove zero variance predictors
degen<- nearZeroVar(changer_data)
changer_new<- changer_data[,-degen]

# Remove highly correlated variables of 75%
corr<- cor(changer_new[,1:1094])
high_corr <- findCorrelation(corr, cutoff = 0.75)
changer_new<- changer_new[,-high_corr]

# Create vector to store 216 values
# For each column of predictors (1 to 216), calculate skew and store in skewed
skewed<- length(216)
for(i in 1:216){
  skewed[i]<- skewness(changer_new[,i], na.rm = TRUE, type = 1)
}
# See where most of the data falls in range
boxplot(skewed)
```

```{r}
# Group data by class
changer_new %>%
  group_by(Class) %>%
  summarise(n = n()) %>%
  gt::gt()
```

```{r}
set.seed(720)
# Create stratified random splits of the data (based on the classes)
trainingRows <- createDataPartition(changer_new$Class, p = .5, list = FALSE) 

# Subset data into train and test
train <- changer_new[trainingRows, ]
test <- changer_new[-trainingRows, ]

# Create control method for cross validation
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)
```

#### Create a functions that train ML models with different parameters
```{r}
# Method
class_function<- function(method){
  model<- train(x = train[,1:216], 
               y = train$Class,
               method = method,
               preProc = c("center", "scale"),
               metric = "ROC",
               trControl = ctrl)
}

# Method and Grid
class_grid<- function(method, grid){
  model<- train(x = train[,1:216], 
               y = train$Class,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               metric = "ROC",
               trControl = ctrl)
}

# Method and Length
class_length<- function(method, length){
  model<- train(x = train[,1:216], 
               y = train$Class,
               method = method,
               preProc = c("center", "scale"),
               tuneLength = length,
               metric = "ROC",
               trControl = ctrl)
}

# Method and Bagging number
class_bag<- function(method, nbagg){
  model<- train(x = train[,1:216], 
               y = train$Class,
               method = method,
               preProc = c("center", "scale"),
               nbagg = nbagg,
               metric = "ROC",
               trControl = ctrl)
}

# Method and Number of trees
class_rf<- function(method, grid, ntree){
  model<- train(x = train[,1:216], 
               y = train$Class,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               ntree = ntree,
               metric = "ROC",
               trControl = ctrl)
}

```

#### Train the ML models with different parameters defined in grids
```{r}
set.seed(720)

# Penalized Models based on  regularization parameters
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                        lambda = seq(.01, .2, length = 10))

# DELETED: nsc_grid<- data.frame(threshold = seq(0, 25, length = 30))

# Mixture Distriminant Analysis
mda_grid<- data.frame(subclasses=1)

# Regularization parameter on a log scale (e.g. 0.001, 0.01, ... 10)

# Neural Network
nnet_grid <- expand.grid(size=1:4, decay=c(0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4))

# SVM
sigmaEst <- kernlab::sigest(as.matrix(train[,1:216]))
svmgrid <- expand.grid(sigma = sigmaEst, C = 2^seq(-4,+4))

# RF Tuning parameter
mtryValues <- data.frame(mtry = seq(1,10,1))

# Generalized Linear Model: Logistic Regression
lr<- class_function("glm")

# Linear Discriminant Analysis
lda<- class_function("lda")

# ???? with ENET model?
glmn<- class_grid("glmnet", glmnGrid)

# Nearest Shrunken Centroids with PAM model
nsc<- class_grid("pam", nsc_grid)

# DELETED: mda<- class_grid("mda", mda_grid)

# SVM with radial
svmR<- class_grid("svmRadial", svmgrid)

# K Nearest Neighbors
knn<- class_length("knn", 20)

# Bagging
bag<- class_bag("treebag", 50)

# Random Forest
rf<- class_rf("rf", mtryValues, 100)
```

#### Evaluating Predictive Performance
```{r}
# Create a function that computes ROC curves of trained models
model_roc <- function(model){
  roc(response = model$pred$obs,
             predictor = model$pred$Changer,
             levels = rev(levels(model$pred$obs)))
}

model_roc(lr)
model_roc(lda)
model_roc(glmn)
model_roc(nsc)
model_roc(svmR)
model_roc(knn)
model_roc(bag)
model_roc(rf)


# Compare Models
train_re<- resamples(list(
  lr = lr,
  lda = lda,
  glmn = glmn,
  nsc = nsc,
  svmR = svmR,
  knn = knn,
  bag = bag,
  rf = rf
))

dotplot(train_re, metric = "ROC")
```
